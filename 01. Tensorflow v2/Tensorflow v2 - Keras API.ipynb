{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras API - Tensorflow v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is the high level API for TensorFlow\n",
    " \n",
    " - If you're an engineer, Keras provides you with reusable blocks such as layers, metrics, training loops, to support common use cases. It provides a high-level user experience that's accessible and productive\n",
    " - If you're a researcher, you may prefer not to use these built-in blocks such as layers and training loops, and instead create your own. Of course, Keras allows you to do this. In this case, Keras provides you with templates for the blocks you write, it provides you with structure, with an API standard for things like Layers and Metrics. This structure makes your code easy to share with others and easy to integrate in production workflows\n",
    " - The same is true for library developers: TensorFlow is a large ecosystem. It has many different libraries. In order for different libraries to be able to talk to each other and share components, they need to follow an API standard. That's what Keras provides\n",
    "\n",
    "Crucially, Keras brings high-level UX and low-level flexibility together fluently: you no longer have on one hand, a high-level API that's easy to use but inflexible, and on the other hand a low-level API that's flexible but only approachable by experts. Instead, you have a spectrum of workflows, from the very high-level to the very low-level. Workflows that are all compatible because they're built on top of the same concepts and objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The base Layer class\n",
    "\n",
    "All layers are pretty much derived from this class only. A Layer will encapsulate a state and computation. State being weights/bias and computation being forward pass (defined under call method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:21.890660Z",
     "start_time": "2020-11-19T18:16:07.688321Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Linear(Layer):\n",
    "    \n",
    "    \"Implementation of y = w.x + b\"\n",
    "\n",
    "    def __init__(self, units = 32, input_dim = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        # Initializing the weight initializing scheme - Random Normal Distribution\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        \n",
    "        # Declaring the Weight Matrix using the scheme, we give the shape (dimensions), no of units \n",
    "        # and specify whether we can update them or not using the trainable Flag\n",
    "        self.w = tf.Variable(initial_value = w_init(shape = (input_dim, units), dtype = 'float32'), trainable = True)\n",
    "        \n",
    "        # Initializing the bias initializing scheme - Zeros\n",
    "        b_init = tf.zeros_initializer()\n",
    "        \n",
    "        # Declaring the Bias Matrix using the scheme, we give the shape (dimensions) \n",
    "        # and specify whether we can update them or not using the trainable Flag \n",
    "        self.b = tf.Variable(initial_value = b_init(shape = (units,), dtype = 'float32'), trainable = True)\n",
    "    \n",
    "    # Forward pass function. This function is called whenever the forward pass is happening\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Defining the Matrix Multiplication operation - w.x + b\n",
    "        return tf.matmul(inputs, self.w) + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:22.112267Z",
     "start_time": "2020-11-19T18:16:21.900633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating the layer with 4 units with an input layer dimension of 2\n",
    "linear_layer = Linear(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:22.299774Z",
     "start_time": "2020-11-19T18:16:22.133213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calling the function \n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert y.shape == (2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Layer class takes care of tracking the weights assigned to it as attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:22.347639Z",
     "start_time": "2020-11-19T18:16:22.317722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Weights are automatically tracked under the weights property\n",
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way of initializing the weights\n",
    "\n",
    "Instead of doing this way,\n",
    " - w_init = tf.random_normal_initializer()\n",
    " - self.w = tf.Variable(initial_value = w_init(shape = shape, dtype = 'float32'))\n",
    "\n",
    "We can do it shortly in this way,\n",
    " - self.w = self.add_weight(shape = shape, initializer = 'random_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can create the Layer in a lazy way and eliminate the input dimension from the constructor. We can have a build function for specifically initializing the weights with a specific dimension.\n",
    "\n",
    "Here the build function gets called automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:22.445378Z",
     "start_time": "2020-11-19T18:16:22.366590Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    \"Implementation of y = w.x + b\"\n",
    "    \n",
    "    def __init__(self, units = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Initialization of weights with the given input shape\n",
    "        self.w = self.add_weight(shape = (input_shape[-1], self.units), initializer = 'random_normal', trainable = True)\n",
    "        \n",
    "        # Initialization of bias with the given input shape\n",
    "        self.b = self.add_weight(shape = (self.units,), initializer = 'random_normal', trainable = True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Forward pass function. This function is called whenever the forward pass is happening \n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:22.505218Z",
     "start_time": "2020-11-19T18:16:22.474307Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the Layer\n",
    "linear_layer = Linear(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:22.582012Z",
     "start_time": "2020-11-19T18:16:22.522173Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will also call build(input_shape) and create the weights\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert len(linear_layer.weights) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable & Non-Trainable Weights\n",
    "\n",
    "We use the trainable flag to freeze and unfreeze the weights in a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:22.690723Z",
     "start_time": "2020-11-19T18:16:22.619911Z"
    }
   },
   "outputs": [],
   "source": [
    "class ComputeSum(Layer):\n",
    "    \n",
    "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        \n",
    "        # Create a non-trainable weight\n",
    "        self.total = tf.Variable(initial_value = tf.zeros((input_dim,)), trainable = False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Add the inputs and compute the sum\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:22.816388Z",
     "start_time": "2020-11-19T18:16:22.714659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Layer\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  \n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  \n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing multiple Layers into bigger computation block of layers\n",
    "\n",
    "Layers can be recursively nested to create bigger computation blocks. Each layer will track the weights of its sublayers (both trainable and non-trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:22.895174Z",
     "start_time": "2020-11-19T18:16:22.833342Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(Layer):\n",
    "    \n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    # Forward Pass function    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Input Layer with ReLU activation\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Hidden Layer with ReLU activation\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:22.986932Z",
     "start_time": "2020-11-19T18:16:22.907144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the block\n",
    "block = MLP()\n",
    "\n",
    "# The first call to the block object will create the weights\n",
    "y = block(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Weights are recursively tracked\n",
    "assert len(block.weights) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model using Keras Sub-Classing technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.080680Z",
     "start_time": "2020-11-19T18:16:23.012862Z"
    }
   },
   "outputs": [],
   "source": [
    "#Keras Custom-Class\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(MyModel, self).__init__()\n",
    "   \n",
    "    #Define your layers here\n",
    "        inputs = tf.keras.Input(shape = (28, 28))  \n",
    "        self.l1 = tf.keras.layers.Flatten()\n",
    "        self.l2 = tf.keras.layers.Dense(512, activation = 'relu', name = 'dense1')\n",
    "        self.l3 = tf.keras.layers.Dropout(0.2)\n",
    "        self.final = tf.keras.layers.Dense(10, activation = tf.nn.softmax, name = 'output1')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "    #Define the forward pass\n",
    "        x = self.l1(inputs)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.348963Z",
     "start_time": "2020-11-19T18:16:23.094644Z"
    }
   },
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "model = MyModel()\n",
    "model.compile(optimizer= tf.keras.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#model.summary()\n",
    "#model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model using Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.555972Z",
     "start_time": "2020-11-19T18:16:23.363923Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using Keras Functional API\n",
    "inputs = tf.keras.Input(shape = (28,28))  \n",
    "x = tf.keras.layers.Flatten()(inputs)\n",
    "x = tf.keras.layers.Dense(512, activation='relu', name = 'dense2')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "final = tf.keras.layers.Dense(10, activation = tf.nn.softmax, name = 'output2')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.699586Z",
     "start_time": "2020-11-19T18:16:23.565943Z"
    }
   },
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "model_1 = tf.keras.Model(inputs = inputs, outputs = final)\n",
    "model_1.compile(optimizer= tf.keras.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#model_1.summary()\n",
    "#model_1.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model using Sequential : Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.858161Z",
     "start_time": "2020-11-19T18:16:23.716549Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using Sequential \n",
    "model_2 = tf.keras.models.Sequential([\n",
    " tf.keras.layers.Flatten(),\n",
    " tf.keras.layers.Dense(512, activation = tf.nn.relu, name = 'dense2'),\n",
    " tf.keras.layers.Dropout(0.2),\n",
    " tf.keras.layers.Dense(10, activation = tf.nn.softmax, name = 'output2')\n",
    "])\n",
    "model_2.compile(optimizer= tf.keras.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#model_2.summary()\n",
    "#model_2.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model using Sequential : Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.999780Z",
     "start_time": "2020-11-19T18:16:23.872123Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using Sequential\n",
    "model_3 = tf.keras.models.Sequential()\n",
    "model_3.add(tf.keras.layers.Flatten())\n",
    "model_3.add(tf.keras.layers.Dense(512, activation='relu', name = 'dense3'))\n",
    "model_3.add(tf.keras.layers.Dropout(0.2))\n",
    "model_3.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax, name = 'output3'))\n",
    "model_3.compile(optimizer= tf.keras.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#model_3.summary()\n",
    "#model_3.fit(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
