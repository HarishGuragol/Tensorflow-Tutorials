{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras API - Tensorflow v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is the high level API for TensorFlow\n",
    " \n",
    "- If you're an engineer, Keras provides you with reusable blocks such as layers, metrics, training loops, to support common use cases. It provides a high-level user experience that's accessible and productive\n",
    "- If you're a researcher, you may prefer not to use these built-in blocks such as layers and training loops, and instead create your own. Of course, Keras allows you to do this. In this case, Keras provides you with templates for the blocks you write, it provides you with structure, with an API standard for things like Layers and Metrics. This structure makes your code easy to share with others and easy to integrate in production workflows\n",
    "- The same is true for library developers: TensorFlow is a large ecosystem. It has many different libraries. In order for different libraries to be able to talk to each other and share components, they need to follow an API standard. That's what Keras provides\n",
    "\n",
    "Crucially, Keras brings high-level UX and low-level flexibility together fluently: you no longer have on one hand, a high-level API that's easy to use but inflexible, and on the other hand a low-level API that's flexible but only approachable by experts. Instead, you have a spectrum of workflows, from the very high-level to the very low-level. Workflows that are all compatible because they're built on top of the same concepts and objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The base Layer class\n",
    "\n",
    "All layers are pretty much derived from this class only. A Layer will encapsulate a state and computation. State being weights/bias and computation being forward pass (defined under call method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:42.485362Z",
     "start_time": "2020-11-19T18:32:29.373239Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Linear(Layer):\n",
    "    \n",
    "    \"Implementation of y = w.x + b\"\n",
    "\n",
    "    def __init__(self, units = 32, input_dim = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        # Initializing the weight initializing scheme - Random Normal Distribution\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        \n",
    "        # Declaring the Weight Matrix using the scheme, we give the shape (dimensions), no of units \n",
    "        # and specify whether we can update them or not using the trainable Flag\n",
    "        self.w = tf.Variable(initial_value = w_init(shape = (input_dim, units), dtype = 'float32'), trainable = True)\n",
    "        \n",
    "        # Initializing the bias initializing scheme - Zeros\n",
    "        b_init = tf.zeros_initializer()\n",
    "        \n",
    "        # Declaring the Bias Matrix using the scheme, we give the shape (dimensions) \n",
    "        # and specify whether we can update them or not using the trainable Flag \n",
    "        self.b = tf.Variable(initial_value = b_init(shape = (units,), dtype = 'float32'), trainable = True)\n",
    "    \n",
    "    # Forward pass function. This function is called whenever the forward pass is happening\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Defining the Matrix Multiplication operation - w.x + b\n",
    "        return tf.matmul(inputs, self.w) + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:42.863348Z",
     "start_time": "2020-11-19T18:32:42.614015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating the layer with 4 units with an input layer dimension of 2\n",
    "linear_layer = Linear(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:43.212415Z",
     "start_time": "2020-11-19T18:32:42.972059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calling the function \n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert y.shape == (2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Layer class takes care of tracking the weights assigned to it as attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:43.400911Z",
     "start_time": "2020-11-19T18:32:43.367999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Weights are automatically tracked under the weights property\n",
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way of initializing the weights\n",
    "\n",
    "Instead of doing this way,\n",
    " - w_init = tf.random_normal_initializer()\n",
    " - self.w = tf.Variable(initial_value = w_init(shape = shape, dtype = 'float32'))\n",
    "\n",
    "We can do it shortly in this way,\n",
    " - self.w = self.add_weight(shape = shape, initializer = 'random_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can create the Layer in a lazy way and eliminate the input dimension from the constructor. We can have a build function for specifically initializing the weights with a specific dimension.\n",
    "\n",
    "Here the build function gets called automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:43.543536Z",
     "start_time": "2020-11-19T18:32:43.492669Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    \"Implementation of y = w.x + b\"\n",
    "    \n",
    "    def __init__(self, units = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Initialization of weights with the given input shape\n",
    "        self.w = self.add_weight(shape = (input_shape[-1], self.units), initializer = 'random_normal', trainable = True)\n",
    "        \n",
    "        # Initialization of bias with the given input shape\n",
    "        self.b = self.add_weight(shape = (self.units,), initializer = 'random_normal', trainable = True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Forward pass function. This function is called whenever the forward pass is happening \n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:43.639275Z",
     "start_time": "2020-11-19T18:32:43.619328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the Layer\n",
    "linear_layer = Linear(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:43.764942Z",
     "start_time": "2020-11-19T18:32:43.715071Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will also call build(input_shape) and create the weights\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert len(linear_layer.weights) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable & Non-Trainable Weights\n",
    "\n",
    "We use the trainable flag to freeze and unfreeze the weights in a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:43.890601Z",
     "start_time": "2020-11-19T18:32:43.854700Z"
    }
   },
   "outputs": [],
   "source": [
    "class ComputeSum(Layer):\n",
    "    \n",
    "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        \n",
    "        # Create a non-trainable weight\n",
    "        self.total = tf.Variable(initial_value = tf.zeros((input_dim,)), trainable = False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Add the inputs and compute the sum\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:44.034217Z",
     "start_time": "2020-11-19T18:32:43.979364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Layer\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  \n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  \n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing multiple Layers into bigger computation block of layers\n",
    "\n",
    "Layers can be recursively nested to create bigger computation blocks. Each layer will track the weights of its sublayers (both trainable and non-trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:44.144923Z",
     "start_time": "2020-11-19T18:32:44.107024Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(Layer):\n",
    "    \n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    # Forward Pass function    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Input Layer with ReLU activation\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Hidden Layer with ReLU activation\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:44.285545Z",
     "start_time": "2020-11-19T18:32:44.216730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the block\n",
    "block = MLP()\n",
    "\n",
    "# The first call to the block object will create the weights\n",
    "y = block(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Weights are recursively tracked\n",
    "assert len(block.weights) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Built-In Layers\n",
    "\n",
    "Keras provides us a wide range of [layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/) for easy and quick prototyping of Deep Neural Networks. Few examples are : \n",
    "\n",
    "- Attention \n",
    "- Activation\n",
    "- Average, Max, Global Average Pooling\n",
    "- BatchNormalization\n",
    "- Bidirectional\n",
    "- Conv1D, Conv2D\n",
    "- Conv1DTranspose, Conv2DTranspose\n",
    "- Dense\n",
    "- Dropout\n",
    "- DepthwiseConvolutions, Separateable convolutions\n",
    "- LSTM, GRU (with built-in cuDNN acceleration) to name a few important ones\n",
    "\n",
    "Keras follows the principles of exposing good default configurations, so that layers will work fine out of the box for most use cases if you leave keyword arguments to their default value. For instance, the LSTM layer uses an orthogonal recurrent matrix initializer by default, and initializes the forget gate bias to one by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training argument in call\n",
    "\n",
    "Some layers, in particular the BatchNormalization layer and the Dropout layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a training (boolean) argument in the call method.\n",
    "\n",
    "By exposing this argument in call, you enable the built-in training and evaluation loops (e.g. fit) to correctly use the layer in training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:32:44.412207Z",
     "start_time": "2020-11-19T18:32:44.360347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the Dropout Layer\n",
    "class Dropout(Layer):\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training = None):\n",
    "        \n",
    "        if training:            \n",
    "            # Dropout enabled computation\n",
    "            return tf.nn.dropout(inputs, rate = self.rate)\n",
    "        \n",
    "        # Droupout disabled output\n",
    "        return inputs\n",
    "\n",
    "class MLPWithDropout(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        \n",
    "        self.linear_1 = Linear(32)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs, training = None):\n",
    "          \n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x, training = training)\n",
    "        \n",
    "        return self.linear_3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:33:33.411320Z",
     "start_time": "2020-11-19T18:33:33.351485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the network with Dropout \n",
    "mlp = MLPWithDropout()\n",
    "\n",
    "y_train = mlp(tf.ones((2, 2)), training = True)\n",
    "y_test = mlp(tf.ones((2, 2)), training = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Models using Functional API\n",
    "\n",
    "To build deep learning models, we don't have to use the tedious OOP way everytime. Layers can also be composed functionally using the Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:41:03.769208Z",
     "start_time": "2020-11-19T18:41:03.359340Z"
    }
   },
   "outputs": [],
   "source": [
    "# We use an `Input` object to describe the shape and dtype of the inputs\n",
    "# The shape argument is per-sample; it does not include the batch size\n",
    "# The functional API focused on defining per-sample transformations\n",
    "# The model we create will automatically batch the per-sample transformations, so that it can be called on batches of data\n",
    "inputs = tf.keras.Input(shape = (16,))\n",
    "\n",
    "# We call layers on these \"type\" objects and they return updated types (new shapes/dtypes)\n",
    "# We are reusing the Linear layer we defined earlier\n",
    "x = Linear(32)(inputs)\n",
    "\n",
    "# We are reusing the Dropout layer we defined earlier\n",
    "x = Dropout(0.5)(x) \n",
    "outputs = Linear(10)(x)\n",
    "\n",
    "# A functional Model can be defined by specifying inputs and outputs\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# A functional model already has weights, before being called on any data\n",
    "# That's because we defined its input shape in advance (in Input)\n",
    "assert len(model.weights) == 4\n",
    "\n",
    "# Let's call our model on some data\n",
    "y = model(tf.ones((2, 16)))\n",
    "assert y.shape == (2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Functional API tends to be more concise than subclassing, and provides a few other advantages. Key differences between ways of defining the models via Model Sub-classing and Functional API way are explained in [this](https://medium.com/tensorflow/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021) blog post.\n",
    "\n",
    "Learn more about the Functional API [here](https://www.tensorflow.org/guide/keras/functional).In your research workflows, you may often find yourself mix-and-matching with Sub-Classing & Functional Ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Models using the Sequential Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T19:24:02.177340Z",
     "start_time": "2020-11-19T19:24:02.035767Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential([Linear(32), Dropout(0.5), Linear(10)])\n",
    "\n",
    "y = model(tf.ones((2, 16)))\n",
    "assert y.shape == (2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of defining a model using Keras Sub-Classing technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:36:12.331295Z",
     "start_time": "2020-11-19T18:36:12.297391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keras Custom-Class\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(MyModel, self).__init__()\n",
    "   \n",
    "        # Define the layers here\n",
    "        inputs = tf.keras.Input(shape = (28, 28))  \n",
    "        self.l1 = tf.keras.layers.Flatten()\n",
    "        self.l2 = tf.keras.layers.Dense(512, activation = 'relu', name = 'dense1')\n",
    "        self.l3 = tf.keras.layers.Dropout(0.2)\n",
    "        self.final = tf.keras.layers.Dense(10, activation = tf.nn.softmax, name = 'output1')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Define the forward pass\n",
    "        x = self.l1(inputs)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        \n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.348963Z",
     "start_time": "2020-11-19T18:16:23.094644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MyModel()\n",
    "\n",
    "model.compile(optimizer= tf.keras.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Summary of the Neural Network Model\n",
    "#model.summary()\n",
    "\n",
    "# Training the Model using the fit function passing the parameters X - {params} and Y - {labels}\n",
    "#model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model using Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.555972Z",
     "start_time": "2020-11-19T18:16:23.363923Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using Keras Functional API\n",
    "inputs = tf.keras.Input(shape = (28,28))  \n",
    "x = tf.keras.layers.Flatten()(inputs)\n",
    "x = tf.keras.layers.Dense(512, activation='relu', name = 'dense2')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "final = tf.keras.layers.Dense(10, activation = tf.nn.softmax, name = 'output2')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.699586Z",
     "start_time": "2020-11-19T18:16:23.565943Z"
    }
   },
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "model_1 = tf.keras.Model(inputs = inputs, outputs = final)\n",
    "model_1.compile(optimizer= tf.keras.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#model_1.summary()\n",
    "#model_1.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model using Sequential : Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.858161Z",
     "start_time": "2020-11-19T18:16:23.716549Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using Sequential \n",
    "model_2 = tf.keras.models.Sequential([\n",
    " tf.keras.layers.Flatten(),\n",
    " tf.keras.layers.Dense(512, activation = tf.nn.relu, name = 'dense2'),\n",
    " tf.keras.layers.Dropout(0.2),\n",
    " tf.keras.layers.Dense(10, activation = tf.nn.softmax, name = 'output2')\n",
    "])\n",
    "model_2.compile(optimizer= tf.keras.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#model_2.summary()\n",
    "#model_2.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model using Sequential : Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:16:23.999780Z",
     "start_time": "2020-11-19T18:16:23.872123Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using Sequential\n",
    "model_3 = tf.keras.models.Sequential()\n",
    "model_3.add(tf.keras.layers.Flatten())\n",
    "model_3.add(tf.keras.layers.Dense(512, activation='relu', name = 'dense3'))\n",
    "model_3.add(tf.keras.layers.Dropout(0.2))\n",
    "model_3.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax, name = 'output3'))\n",
    "model_3.compile(optimizer= tf.keras.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#model_3.summary()\n",
    "#model_3.fit(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
